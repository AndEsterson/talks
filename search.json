[
  {
    "objectID": "kubeflow-trainer-proposal/index.html#kubeflow-trainer-out-of-the-box",
    "href": "kubeflow-trainer-proposal/index.html#kubeflow-trainer-out-of-the-box",
    "title": "Kubeflow Trainer Workflow",
    "section": "Kubeflow Trainer out of the box",
    "text": "Kubeflow Trainer out of the box\n\n\n\n\n\nflowchart LR\n  A(user submits a job .) --&gt; B(TrainJob created .)\n  B --&gt; C(Pods are .\n          created)\n  C --&gt; D(Pods run to \n          completion/failure ..)\n  D --&gt; E(Trainjob is updated .. \n          to reflect\n          completion/failure ..)"
  },
  {
    "objectID": "kubeflow-trainer-proposal/index.html#some-problems",
    "href": "kubeflow-trainer-proposal/index.html#some-problems",
    "title": "Kubeflow Trainer Workflow",
    "section": "Some problems",
    "text": "Some problems\n\nWhat if the pods need to queue for resources? We have no gang scheduling, no shared resource pools, and no fairness scheduling\nHow does the user see the logs for the job? Currently they must manually watch the pod logs in real time\nWhat happens to stale Trainjobs? Currently they stay there forever, steadily increaasing the size of the etcd database"
  },
  {
    "objectID": "kubeflow-trainer-proposal/index.html#updated-kubeflow-trainer",
    "href": "kubeflow-trainer-proposal/index.html#updated-kubeflow-trainer",
    "title": "Kubeflow Trainer Workflow",
    "section": "Updated Kubeflow Trainer",
    "text": "Updated Kubeflow Trainer\n\n\n\n\n\n%%{init: {\n\"flowchart\": {\n  \"rankSpacing\": 40\n}\n}}%%\nflowchart LR\n  classDef withmargines fill-opacity:0.0,color:#FFFFFF,stroke-width:0px;\n  A(User submits a job .) --&gt; B(TrainJob created .)\n  C --&gt; D(Pods run to \n          completion/failure .)\n  D --&gt; E(Trainjob is updated ..\n          to reflect \n          completion/failure ..)\n  B --&gt; Z\n  Z2 --&gt; C(Pods are created .)\n  Z3 --&gt; C\n  C --&gt; X\n  C --&gt; W\n  E --&gt; Y(Delete jobs which \n          have been finished .\n          for over 1 week)\n\n  subgraph volcano [Volcano\n                    Scheduler .]\n    direction LR\n    Z(Volcano PodGroup ..\n      created)\n    Z1(Check enough resources ..\n      available to schedule\n      the TrainJob)\n    Z2(Borrow resources\n      from other pools\n      if required and allowed ..)\n    Z3(Reclaim resources\n      from other pools\n      if required and possible ..)\n    \n    Z --&gt; Z1\n    Z1 --&gt; Z2\n    Z1 --&gt; Z3\n  end\n\n  subgraph loki [Loki \n                Log persistence .]\n    foo1[\"&lt;p style='width:100px;height:0px;margin:0'&gt;Invisible !!!/p&gt;\"]:::withmargines\n    foo2[\"&lt;p style='width:100px;height:0px;margin:0'&gt;Invisible !!!/p&gt;\"]:::withmargines\n    X1[(Logs are forwarded ..\n      from Alloy to\n      the Loki database)]\n      X2{{ Grafana queries Loki\n      for admin dashboards .. }}\n      X3{{ User facing \n      web-app queries Loki .. }}\n      X4{{ User directly .\n      queries Loki }}\n      X( Pod logs are collected ..\n      by Grafana Alloy )\n    X2 --&gt; X1\n    X3 ---&gt; X1\n    X4 ---&gt; X1\n    X --&gt; X1\n  end\n\n  subgraph RDSS .\n    W[(data written to\n        user's RDSS project ..)]\n  end\n  subgraph kyverno [Kyverno\n                    cleanup .]\n    Y\n  end"
  },
  {
    "objectID": "kubeflow-demo/index.html#kubeflow",
    "href": "kubeflow-demo/index.html#kubeflow",
    "title": "Kubeflow demo",
    "section": "Kubeflow",
    "text": "Kubeflow"
  },
  {
    "objectID": "kubeflow-demo/index.html#benefits",
    "href": "kubeflow-demo/index.html#benefits",
    "title": "Kubeflow demo",
    "section": "Benefits",
    "text": "Benefits\n\nKubernetes is very flexible! We can add/remove/change the servies we offer while keeping the same underlying core. In a few years we might offer a completely different set of services!\nSince Kubernetes doesn’t make a fundamental distinction between short-lived jobs and permanent services, we can easily integrate e.g jupyter notebooks\nCaching logic in Kubeflow pipelines lets us cut down on resource usage, and lets researchers build clear and reproducable pipelines"
  },
  {
    "objectID": "kubeflow-demo/index.html#what-the-lifecycle-of-a-job-would-ideally-look-like",
    "href": "kubeflow-demo/index.html#what-the-lifecycle-of-a-job-would-ideally-look-like",
    "title": "Kubeflow demo",
    "section": "What the lifecycle of a job would ideally look like",
    "text": "What the lifecycle of a job would ideally look like\n\nA job is submitted by a user, and is validated as reasonable-looking\nThe job then enters a queue, and the user is informed about its progress\nThe job then graduates from queueing to running, and the user can view logs\nThe job then finishes in finite time with either success or failure, with a reason given for failure"
  },
  {
    "objectID": "kubeflow-demo/index.html#what-the-lifecycle-of-a-job-actually-looks-like-for-now",
    "href": "kubeflow-demo/index.html#what-the-lifecycle-of-a-job-actually-looks-like-for-now",
    "title": "Kubeflow demo",
    "section": "What the lifecycle of a job actually looks like (for now!)",
    "text": "What the lifecycle of a job actually looks like (for now!)\n\nThe user can submit multiple types of resources (e.g notebooks, pipeline runs, Kubeflow Trainer jobs, inference services), which then lead to the creation of one or multiple Pod resource(s)\nIf the Pod has to wait before being scheduled to a node, the user isn’t automatically informed about this, and the reason isn’t always clear to a user (e.g there are some edge cases where user specified config will cause the Pod to never schedule)\nOnce the job is running, the user can view logs, but the interface is clunky\nWe don’t currently enforce any kind of maximum runtime, we may be able to use the for this."
  },
  {
    "objectID": "kubeflow-demo/index.html#how-might-we-solve-these-problems",
    "href": "kubeflow-demo/index.html#how-might-we-solve-these-problems",
    "title": "Kubeflow demo",
    "section": "How might we solve these problems?",
    "text": "How might we solve these problems?\n\nWe deploy Prometheus/Grafana, so we could export logs and cluster Events in a sanitised way to a user specific dashboard. We could also fire user-level alerts to email/slack/teams/something else.\nWe can also use Kyverno to validate inputs, so that user jobs fail fast (but doing this for every Kubeflow component might get difficult)\nWe can use tools like the descheduler to enforce runtime limits (still a lot of work to implement this in a flexible way)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Talks",
    "section": "",
    "text": "Using Kubeflow to Build HPC Tailored for the AI/ML Lifecycle\n\n\n\n\n\n\n\n\n\n\n\nAndrew Esterson\n\n\n\n\n\n\n\n\n\n\n\n\nKubeflow Trainer Workflow\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2025\n\n\nAndrew Esterson\n\n\n\n\n\n\n\n\n\n\n\n\nKubeflow demo\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2025\n\n\nAndrew Esterson\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "hpc-ai/index.html#when-we-say-ml-lifecycle-what-do-we-mean",
    "href": "hpc-ai/index.html#when-we-say-ml-lifecycle-what-do-we-mean",
    "title": "Using Kubeflow to Build HPC Tailored for the AI/ML Lifecycle",
    "section": "When we say ML Lifecycle, what do we mean?",
    "text": "When we say ML Lifecycle, what do we mean?\n\n\n\n\n\nflowchart LR\n  A(Foundation dataset) --&gt; B(Foundation model training)\n  B --&gt; C(Model finetuning)\n  H(Finetuning dataset) --&gt; C\n  C --&gt; G[Model evaluation]\n  G --&gt; C\n  C --&gt; D{Serve model}\n  D --&gt; E[Research output]\n  D --&gt; F[Ad-hoc testing]"
  },
  {
    "objectID": "hpc-ai/index.html#our-platform",
    "href": "hpc-ai/index.html#our-platform",
    "title": "Using Kubeflow to Build HPC Tailored for the AI/ML Lifecycle",
    "section": "Our platform",
    "text": "Our platform\n\nFor UCL researchers (and external users in the long term)\nUses Kubeflow, an Open Source project designed to deploy into a Kubernetes cluster\nOur cluster currently has ~8TB of memory, ~600 cpu cores, &gt;4TB total GPU VRAM - mostly NVIDIA A100 40GB and 80GB with 8-way NVLinks\nBuilt on top of a larger UCL on-prem cluster (which means the total resources above are flexible)"
  },
  {
    "objectID": "hpc-ai/index.html#so-what-does-it-do",
    "href": "hpc-ai/index.html#so-what-does-it-do",
    "title": "Using Kubeflow to Build HPC Tailored for the AI/ML Lifecycle",
    "section": "So what does it do?",
    "text": "So what does it do?\nConverts this\n\n\n\n\n\nflowchart LR\n  A(Foundation dataset) --&gt; B(Foundation model training)\n  B --&gt; C(Model finetuning)\n  H(Finetuning dataset) --&gt; C\n  C --&gt; G[Model evaluation]\n  G --&gt; C\n  C --&gt; D{Serve model}\n  D --&gt; E[Research output]\n  D --&gt; F[Ad-hoc testing]"
  },
  {
    "objectID": "hpc-ai/index.html#so-what-does-it-do-1",
    "href": "hpc-ai/index.html#so-what-does-it-do-1",
    "title": "Using Kubeflow to Build HPC Tailored for the AI/ML Lifecycle",
    "section": "So what does it do?",
    "text": "So what does it do?\nInto this"
  },
  {
    "objectID": "hpc-ai/index.html#workflow",
    "href": "hpc-ai/index.html#workflow",
    "title": "Using Kubeflow to Build HPC Tailored for the AI/ML Lifecycle",
    "section": "Workflow",
    "text": "Workflow\n\n\nsubmit_pipeline.py\n\nimport kfp\nimport kfp.dsl\n\n@kfp.dsl.pipeline\ndef my_pipeline():\n  \"\"\"This function specifies the pipeline spec.\"\"\"\n  ...\n\nkfp_client = kfp.Client()\nkfp_client.create_run_from_pipeline_func(my_pipeline)\n\n\n\nterminal\n\npython3 ./submit_pipeline.py"
  },
  {
    "objectID": "hpc-ai/index.html#workflow-1",
    "href": "hpc-ai/index.html#workflow-1",
    "title": "Using Kubeflow to Build HPC Tailored for the AI/ML Lifecycle",
    "section": "Workflow",
    "text": "Workflow\n\n\nmodel_spec.yaml\n\napiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: my-inference-model\nspec:\n  predictor:\n    model:\n      modelFormat:\n        name: huggingface\n      resources:\n        requests:\n          cpu: \"32\"\n          memory: 180Gi\n          nvidia.com/gpu: \"8\"\n          ephemeral-storage: 400Gi\n      storageUri: s3://models-a-esterson/my-inference-model"
  },
  {
    "objectID": "hpc-ai/index.html#workflow-2",
    "href": "hpc-ai/index.html#workflow-2",
    "title": "Using Kubeflow to Build HPC Tailored for the AI/ML Lifecycle",
    "section": "Workflow",
    "text": "Workflow"
  },
  {
    "objectID": "hpc-ai/index.html#workflow-3",
    "href": "hpc-ai/index.html#workflow-3",
    "title": "Using Kubeflow to Build HPC Tailored for the AI/ML Lifecycle",
    "section": "Workflow",
    "text": "Workflow\n\n\n\n\n\nflowchart LR\n  subgraph S4 [Centralised UCL Storage]\n    A(Foundation dataset)\n    H(Finetuning dataset)\n  end\n\n  A --&gt; B(Foundation model training)\n  H --&gt; C(Model finetuning)\n\n  subgraph S1 [Kubeflow Pipelines]\n    B --&gt; C\n    C --&gt; G[Model evaluation]\n    G --&gt; C\n  end\n\n  subgraph S2 [Kserve]\n    D{Serve model}\n  end\n\n\n  subgraph S3 [Kubeflow Notebooks]\n    F[Ad-hoc testing]\n  end\n\n  C --&gt; D\n  D --&gt; E(Research Output)\n  D --&gt; F\n\n  style S1 fill:#add8e6,fill-opacity:0.25,stroke:#5b9bd5,stroke-width:1px\n  style S2 fill:#90ee90,fill-opacity:0.25,stroke:#70ad47,stroke-width:1px\n  style S3 fill:#f0ee90,fill-opacity:0.25,stroke:#f0ad47,stroke-width:1px\n  style S4 fill:#d9b3ff,fill-opacity:0.25,stroke:#9b59b6,stroke-width:1px"
  },
  {
    "objectID": "hpc-ai/index.html#workflow-4",
    "href": "hpc-ai/index.html#workflow-4",
    "title": "Using Kubeflow to Build HPC Tailored for the AI/ML Lifecycle",
    "section": "Workflow",
    "text": "Workflow\n\nAll of these components share common:\n\nstorage\nresource quotas\nauthentication"
  },
  {
    "objectID": "hpc-ai/index.html#currently-our-users-are",
    "href": "hpc-ai/index.html#currently-our-users-are",
    "title": "Using Kubeflow to Build HPC Tailored for the AI/ML Lifecycle",
    "section": "Currently our users are…",
    "text": "Currently our users are…\n\nServing a model using Kserve to run OCR text recognition without data leaving the UCL network\nFinetuning a language model to use for a RAG application\nTraining foundation vision models for medical imaging\nWe’d like to give access to more users to explore loads more problems!"
  },
  {
    "objectID": "hpc-ai/index.html#challenges",
    "href": "hpc-ai/index.html#challenges",
    "title": "Using Kubeflow to Build HPC Tailored for the AI/ML Lifecycle",
    "section": "Challenges",
    "text": "Challenges\n\nIt takes time for people to adapt to a new framework\n\nWe need to write really good documentation!\n\nUsers can’t be expected to be Kubernetes experts\n\nWe need to abstract away complexity\n\nThe ecosystem for HPC on Kubernetes is still developing\n\nMeaning we have to do more internal plumbing than usual"
  },
  {
    "objectID": "hpc-ai/index.html#where-do-we-go-from-here",
    "href": "hpc-ai/index.html#where-do-we-go-from-here",
    "title": "Using Kubeflow to Build HPC Tailored for the AI/ML Lifecycle",
    "section": "Where do we go from here?",
    "text": "Where do we go from here?\n\nExpanding access to more users so that we can write better docs, fix bugs and improve performance\nPotentially expand the services we offer\nBuild higher-level abstractions and integrations (e.g Github Actions)\nLinking the cluster to a cloud provider so that we can dip into the cloud when demand spikes?"
  }
]